# Chapter 4 - Algorithms

## Least squares for classification

$$
E_{n}(\widetilde{\mathbf{W}})
=\frac{1}{2}\mathrm{Tr}\left\{(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})^{\mathrm{T}}\right\}
\left\{(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})\right\}\\
\widetilde{\mathbf{W}}=(\widetilde{\mathbf{X}}^{\mathrm{T}}\widetilde{\mathbf{X}})^{\mathrm{-1}}\widetilde{\mathbf{X}}^{\mathrm{T}}=\widetilde{\mathbf{X}}^{\dagger}\mathbf{T}
$$

## The perceptron algorithm

$$
\mathbf{w}^{(\tau+1)}=\begin{cases}
\mathbf{w}^{(\tau)}+\phi_{n}t_{n} & \mathbf{x}_{n}\ \text{misclassified}\\
\mathbf{w}^{(\tau)} & \mathbf{x}_{n}\ \text{correctly classified}
\end{cases}
$$

## Logistic regression using iterative reweighted least squares 

$$
\begin{align}
\nabla E(\mathbf{w}) & =\sum_{n=1}^{N}(y_{n}-t_{n})\phi_{n}=\mathbf{\Phi}^{\mathrm{T}}(\mathbf{y-t})\\
\mathbf{H} &= \nabla\nabla E(\mathbf{w})=\sum_{n=1}^{N}y_{n}(1-y_{n})\phi_{n}\phi_{n}^{\mathrm{T}}=\mathbf{\Phi}^{\mathrm{T}} \mathbf{R \Phi}
\end{align}
$$