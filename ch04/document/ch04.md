# Chapter 4 - Algorithms

## Least squares for classification

$$
E_{n}(\widetilde{\mathbf{W}})
=\frac{1}{2}\mathrm{Tr}\left\{(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})^{\mathrm{T}}\right\}
\left\{(\widetilde{\mathbf{X}}\widetilde{\mathbf{W}}-\mathbf{T})\right\}\\
\widetilde{\mathbf{W}}=(\widetilde{\mathbf{X}}^{\mathrm{T}}\widetilde{\mathbf{X}})^{\mathrm{-1}}\widetilde{\mathbf{X}}^{\mathrm{T}}=\widetilde{\mathbf{X}}^{\dagger}\mathbf{T}
$$

## The perceptron algorithm

$$
\mathbf{w}^{(\tau+1)}=\begin{cases}
\mathbf{w}^{(\tau)}+\phi_{n}t_{n} & \mathbf{x}_{n}\ \text{misclassified}\\
\mathbf{w}^{(\tau)} & \mathbf{x}_{n}\ \text{correctly classified}
\end{cases}
$$

## Logistic regression using iterative reweighted least squares 

$$
\begin{align}
\nabla E(\mathbf{w}) & =\sum_{n=1}^{N}(y_{n}-t_{n})\phi_{n}=\mathbf{\Phi}^{\mathrm{T}}(\mathbf{y-t})\\
\mathbf{H} &= \nabla\nabla E(\mathbf{w})=\sum_{n=1}^{N}y_{n}(1-y_{n})\phi_{n}\phi_{n}^{\mathrm{T}}=\mathbf{\Phi}^{\mathrm{T}} \mathbf{R \Phi}
\end{align}
$$

## Fisher's linear discriminant

mean vector:
$$
\begin{align}
\mathbf{m}_{1}&=\frac{1}{N_{1}}\sum_{n\in \mathcal{C}_{1}}\mathbf{x}_{n}\\
\mathbf{m}_{2}&=\frac{1}{N_{2}}\sum_{n\in \mathcal{C}_{2}}\mathbf{x}_{n}
\end{align}
$$
within-class convariance matrix:
$$
\mathbf{S}_{\mathrm{W}}=\sum_{n\in\mathcal{C}_{1}}(\mathbf{x}_{n}-\mathbf{m}_{1})(\mathbf{x}_{n}-\mathbf{m}_{1})^{\mathrm{T}}+\sum_{n\in\mathcal{C}_{2}}(\mathbf{x}_{n}-\mathbf{m}_{2})(\mathbf{x}_{n}-\mathbf{m}_{2})^{\mathrm{T}}
$$
unnormalized $\mathbf{w}$:
$$
\mathbf{w}\propto\mathbf{S}_{\mathrm{W}}^{-1}(\mathbf{m}_{2}-\mathbf{m_{1}})
$$


threshold:

â€‹	goal: be the hyperplane between projections of the two means,
$$
\mathrm{threshold}= \mathbf{w} \cdot \frac{1}{2}(\mathbf{m}_{1}+\mathbf{m}_{2})
$$


